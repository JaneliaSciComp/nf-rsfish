# yaml-language-server: $schema=https://raw.githubusercontent.com/nf-core/modules/master/subworkflows/yaml-schema.json
name: spark_start
description: |
  Starts Spark processing either by spinning up a cluster or setting up
  variables so that processing can run locally as individual jobs
keywords:
  - spark
  - bigdata
  - infrastructure
components: []
input:
  - ch_meta:
      type: tuple
      description: |
        Channel of tuples containing a meta map
        Structure: [ val(meta) ]
  - working_dir:
      type: path
      description: Path shared by workers for logging and jar distribution
  - data_dirs:
      type: path
      description: Paths to be mounted in the Spark workers for data access
  - spark_cluster:
      type: boolean
      description: Whether or not to spin up a Spark cluster
  - spark_workers:
      type: integer
      description: Number of workers in the cluster
  - spark_worker_cores:
      type: integer
      description: Number of cores per Spark worker
  - spark_gb_per_core:
      type: integer
      description: Number of GB of memory per worker core
  - spark_driver_cores:
      type: integer
      description: Number of cores for the Spark driver
  - spark_driver_memory:
      type: string
      description: Memory specification for the Spark driver

output:
  - spark_context:
      type: tuple
      description: |
        The tuple from input ch_meta with the spark_context map appended.
        Structure: [ val(meta), val(spark_context) ]

authors:
  - "@krokicki"
  - "@cgoina"